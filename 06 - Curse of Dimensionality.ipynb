{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "intro_md",
            "metadata": {},
            "source": [
                "# Maldição da Dimensionalidade\n",
                "\n",
                "A **Maldição da Dimensionalidade** (Curse of Dimensionality) refere-se a vários fenômenos que surgem ao analisarmos e organizarmos dados em espaços de alta dimensão (frequentemente com centenas ou milhares de dimensões) que não ocorrem em espaços de baixa dimensão, como o espaço tridimensional físico que experimentamos no dia a dia.\n",
                "\n",
                "Neste notebook, exploraremos dois dos efeitos mais contra-intuitivos e impactantes dessa maldição:\n",
                "1.  O esvaziamento do espaço (a maior parte do volume de um hipercubo concentra-se nos cantos).\n",
                "2.  A concentração de distâncias (todas as distâncias entre pontos tendem a se tornar iguais).\n",
                "\n",
                "Esses fenômenos são fundamentais para entender por que algoritmos baseados em distância, como o K-Means e o KNN, falham em altas dimensões e motivam o uso de técnicas de Redução de Dimensionalidade (que veremos nos próximos notebooks)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "imports",
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from scipy.spatial.distance import pdist, squareform\n",
                "import seaborn as sns\n",
                "\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "%matplotlib inline"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "vol_intro_md",
            "metadata": {},
            "source": [
                "## 1. O Volume da Hiperesfera\n",
                "\n",
                "Considere uma hiperesfera de raio $r=1$ inscrita em um hipercubo de aresta $2r=2$ (centrado na origem). Em 2D, temos um círculo dentro de um quadrado. Em 3D, uma esfera dentro de um cubo.\n",
                "\n",
                "Vamos investigar qual fração do volume do hipercubo é ocupada pela hiperesfera à medida que a dimensão $d$ aumenta.\n",
                "\n",
                "Matematicamente, definimos a razão de volumes como:\n",
                "$$ \\text{Razão} = \\frac{\\text{Volume da Esfera}_d}{\\text{Volume do Cubo}_d} $$\n",
                "\n",
                "Usaremos uma **Simulação de Monte Carlo** para estimar essa razão:\n",
                "1. Geramos pontos aleatórios uniformemente distribuídos dentro do hipercubo $[-1, 1]^d$.\n",
                "2. Contamos quantos desses pontos caem dentro da esfera (distância à origem $\\le 1$).\n",
                "3. A razão de pontos dentro da esfera sobre o total nos dá uma aproximação da razão de volumes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "sphere_sim",
            "metadata": {},
            "outputs": [],
            "source": [
                "def sphere_volume(dim, n_samples=100000):\n",
                "    # Gerar pontos uniformes no hipercubo [-1, 1]^d\n",
                "    X = np.random.uniform(-1, 1, size=(n_samples, dim))\n",
                "    \n",
                "    # Calcular distância euclidiana ao quadrado (mais rápido) para cada ponto\n",
                "    # Se sum(coords^2) <= 1, ponto está dentro da esfera unitária\n",
                "    dist_sq = np.sum(X**2, axis=1)\n",
                "    points_inside = np.sum(dist_sq <= 1.0)\n",
                "    \n",
                "    return points_inside / n_samples\n",
                "\n",
                "dimesoes_teste = [2, 3, 4, 5, 10, 20]\n",
                "ratios = []\n",
                "\n",
                "print(f\"{'Dimensão':<10} | {'Razão (Esfera/Cubo)':<25}\")\n",
                "print(\"-\" * 40)\n",
                "\n",
                "for d in dimesoes_teste:\n",
                "    ratio = sphere_volume(d)\n",
                "    ratios.append(ratio)\n",
                "    print(f\"{d:<10} | {ratio:.5f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "plot_sphere",
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(8, 5))\n",
                "plt.plot(dimesoes_teste, ratios, marker='o', linestyle='-', color='indigo')\n",
                "plt.title('Fração do Volume do Hipercubo Ocupada pela Hiperesfera')\n",
                "plt.xlabel('Dimensão ($d$)')\n",
                "plt.ylabel('Razão Volume Esfera / Volume Cubo')\n",
                "plt.grid(True)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "vol_analysis",
            "metadata": {},
            "source": [
                "### Análise\n",
                "\n",
                "O resultado é surpreendente: em dimensões altas (até mesmo $d=10$ ou $20$), o volume da esfera torna-se insignificante em comparação ao do cubo. Isso significa que **quase todo o volume do espaço de alta dimensão está localizado nos \"cantos\" do hipercubo**, longe do centro.\n",
                "\n",
                "Para algoritmos de Machine Learning, isso implica que os dados tornam-se extremamente **esparsos**. Para cobrir o espaço com a mesma densidade de amostras que temos em 1D, a quantidade de dados necessária cresce exponencialmente com a dimensão."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "dist_conc_intro",
            "metadata": {},
            "source": [
                "## 2. Concentração de Distâncias\n",
                "\n",
                "Outro efeito crítico é o comportamento da Distância Euclidiana. À medida que a dimensão aumenta, a diferença proporcional entre a distância do ponto mais próximo e a do ponto mais distante tende a zero.\n",
                "\n",
                "Sejam $d_{\\min}$ e $d_{\\max}$ as distâncias mínima e máxima de um ponto de consulta para os outros pontos da amostra. Em altas dimensões:\n",
                "\n",
                "$$ \\lim_{d \\to \\infty} \\frac{d_{\\max} - d_{\\min}}{d_{\\min}} \\to 0 $$\n",
                "\n",
                "Isso significa que, do ponto de vista de qualquer amostra, **todos os outros pontos parecem estar aproximadamente à mesma distância**. O conceito de \"vizinho mais próximo\" perde o sentido."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "dist_sim",
            "metadata": {},
            "outputs": [],
            "source": [
                "def analisar_distancias(dims, n_points=500):\n",
                "    plt.figure(figsize=(12, 8))\n",
                "    \n",
                "    for i, d in enumerate(dims):\n",
                "        # Gerar n_points em dimensão d (Distribuição Uniforme [0, 1])\n",
                "        X = np.random.rand(n_points, d)\n",
                "        \n",
                "        # Calcular todas as distâncias par-a-par\n",
                "        dists = pdist(X, metric='euclidean')\n",
                "        \n",
                "        # Calcular métricas\n",
                "        d_min, d_max, d_mean = np.min(dists), np.max(dists), np.mean(dists)\n",
                "        contraste = (d_max - d_min) / d_min\n",
                "        \n",
                "        # Plotar histograma normalizado pela média para facilitar comparação visual\n",
                "        # Isso nos mostra a dispersão relativa\n",
                "        sns.histplot(dists / d_mean, kde=True, label=f'd={d} (Contrast={contraste:.2f})', alpha=0.6, stat='density', element=\"step\")\n",
                "        \n",
                "    plt.title('Distribuição das Distâncias Par-a-Par (Normalizadas pela Média)')\n",
                "    plt.xlabel('Distância / Média')\n",
                "    plt.ylabel('Densidade')\n",
                "    plt.legend()\n",
                "    plt.xlim(0, 3)\n",
                "    plt.show()\n",
                "\n",
                "# Dimensões para teste: de baixa (2D) para muito alta (5000D)\n",
                "dims_to_test = [2, 10, 100, 1000, 5000]\n",
                "analisar_distancias(dims_to_test)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "dist_analysis",
            "metadata": {},
            "source": [
                "### Conclusão sobre Distâncias\n",
                "\n",
                "Observe no gráfico acima como as distribuições se tornam mais estreitas (em relação à média) conforme a dimensão aumenta. \n",
                "\n",
                "- Em **2D** (azul), há uma grande variância: alguns pontos estão muito próximos, outros muito distantes.\n",
                "- Em **5000D** (roxo), o histograma é um pico estreito ao redor da média. A distância para o vizinho mais próximo é quase igual à distância para o vizinho mais distante.\n",
                "\n",
                "**Impacto no Clustering:** O K-Means, que depende de calcular distâncias para centróides, torna-se ineficaz. O algoritmo converge lentamente ou encontra ótimos locais ruins, pois a distinção espacial entre clusters se dissolve."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ortho_intro",
            "metadata": {},
            "source": [
                "## 3. Ortogonalidade Assíntota\n",
                "\n",
                "Uma consequência menos conhecida, mas igualmente importante, da alta dimensionalidade é que **vetores aleatórios tendem a ser ortogonais entre si**.\n",
                "\n",
                "Imagine escolher dois vetores aleatórios partindo da origem em um espaço de alta dimensão. A probabilidade de que eles apontem em direções similares (ângulo próximo de 0°) ou opostas (ângulo próximo de 180°) é extremamente baixa. A grande maioria dos pares formará um ângulo muito próximo de 90°.\n",
                "\n",
                "Isso acontece porque o \"equador\" de uma hiperesfera concentra a maior parte da área da superfície à medida que a dimensão cresce."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ortho_sim",
            "metadata": {},
            "outputs": [],
            "source": [
                "def analisar_ortogonalidade(dims, n_pairs=10000):\n",
                "    plt.figure(figsize=(10, 6))\n",
                "    \n",
                "    for d in dims:\n",
                "        # Gerar pares de vetores aleatórios (distribuição normal funciona bem para direção isotrópica)\n",
                "        u = np.random.randn(n_pairs, d)\n",
                "        v = np.random.randn(n_pairs, d)\n",
                "        \n",
                "        # Calcular cosseno do ângulo: (u . v) / (|u| |v|)\n",
                "        dot_product = np.sum(u * v, axis=1)\n",
                "        norm_u = np.linalg.norm(u, axis=1)\n",
                "        norm_v = np.linalg.norm(v, axis=1)\n",
                "        \n",
                "        # Evitar divisão por zero\n",
                "        valid = (norm_u > 0) & (norm_v > 0)\n",
                "        \n",
                "        cos_theta = dot_product[valid] / (norm_u[valid] * norm_v[valid])\n",
                "        \n",
                "        # Garantir que valores estejam em [-1, 1] para evitar erro numérico no arccos\n",
                "        cos_theta = np.clip(cos_theta, -1.0, 1.0)\n",
                "        \n",
                "        # Converter para graus\n",
                "        angles = np.degrees(np.arccos(cos_theta))\n",
                "        \n",
                "        sns.histplot(angles, kde=True, label=f'd={d}', alpha=0.6, element=\"step\", stat='density')\n",
                "        \n",
                "    plt.title('Distribuição dos Ângulos entre Vetores Aleatórios')\n",
                "    plt.xlabel('Ângulo (graus)')\n",
                "    plt.ylabel('Densidade')\n",
                "    plt.axvline(90, color='red', linestyle='--', alpha=0.5, label='90°')\n",
                "    plt.legend()\n",
                "    plt.xlim(0, 180)\n",
                "    plt.show()\n",
                "\n",
                "analisar_ortogonalidade([2, 10, 100, 1000])"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ortho_analysis",
            "metadata": {},
            "source": [
                "### Análise da Ortogonalidade\n",
                "\n",
                "- Em **2D**, os ângulos estão uniformemente espalhados.\n",
                "- Conforme **d aumenta**, a distribuição se concentra agudamente em torno de **90 graus**.\n",
                "\n",
                "Isso tem implicações profundas: em Espaços Vetoriais de Alta Dimensão (como em Embeddings de palavras de 768 ou 1536 dimensões), quase todos os vetores são ortogonais entre si. Isso pode ser vantajoso (capacidade de armazenar muitas informações distintas quase ortogonalmente) ou problemático (dificuldade em encontrar similaridades significativas se não houver estrutura nos dados)."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "conclusion",
            "metadata": {},
            "source": [
                "## 4. Como lidar com a Maldição?\n",
                "\n",
                "Para combater esses efeitos em datasets de alta dimensão (como imagens, embeddings de texto, dados genômicos), utilizamos técnicas de **Redução de Dimensionalidade**.\n",
                "\n",
                "O objetivo é encontrar um subespaço de menor dimensão (manifold) onde os dados \"vivem\" intrinsecamente, preservando suas estruturas importantes.\n",
                "\n",
                "Nos próximos notebooks, estudaremos as principais técnicas para realizar essa tarefa:\n",
                "1.  **PCA (Análise de Componentes Principais)**: Redução linear que preserva variância global.\n",
                "2.  **t-SNE**: Redução não-linear focada em preservar vizinhanças locais (ótima para visualização).\n",
                "3.  **UMAP**: Alternativa moderna ao t-SNE que equilibra estrutura local e global."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "base",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
