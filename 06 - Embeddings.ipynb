{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "969b2b2e",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "\n",
    "Este notebook introduz o conceito fundamental de *embeddings*, que são representações vetoriais de dados complexos em um espaço de dimensão inferior. Exploraremos como gerar embeddings de texto utilizando modelos de linguagem modernos, como o BERTimbau, e aplicaremos esses vetores em tarefas práticas como cálculo de similaridade e busca semântica. Finalmente, visualizaremos a estrutura desses embeddings de alta dimensão utilizando técnicas de redução de dimensionalidade como t-SNE e UMAP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8881c29b",
   "metadata": {},
   "source": [
    "### Introdução\n",
    "\n",
    "Em aprendizado de máquina, um *embedding* é uma representação de dados complexos e de alta dimensionalidade em um espaço vetorial de dimensão inferior. O objetivo principal é capturar as relações semânticas ou estruturais inerentes aos dados brutos. Em vez de trabalhar com dados esparsos e categóricos (como palavras em um vocabulário), os modelos operam com vetores densos e contínuos.\n",
    "\n",
    "Matematicamente, um embedding é uma função de mapeamento $f: X \\rightarrow \\mathbb{R}^d$, onde $X$ é o espaço dos dados de entrada (e.g., o conjunto de todas as palavras) e $\\mathbb{R}^d$ é um espaço vetorial de dimensão $d$. A propriedade fundamental de um bom embedding é que a distância e a direção entre os vetores no espaço $\\mathbb{R}^d$ correspondam a alguma noção de similaridade no espaço original $X$. Por exemplo, em embeddings de palavras, espera-se que as palavras \"rei\" e \"rainha\" estejam mais próximas no espaço vetorial do que as palavras \"rei\" e \"carro\".\n",
    "\n",
    "Embora os embeddings de texto sejam os mais conhecidos, a técnica é aplicável a diversos domínios:\n",
    "\n",
    "* **Embeddings de Grafos**: Nós em um grafo podem ser representados como vetores que capturam a topologia da rede (e.g., Node2Vec).\n",
    "* **Embeddings de Imagens**: Modelos como Redes Neurais Convolucionais (CNNs) aprendem a extrair vetores de características (*feature vectors*) que representam o conteúdo semântico de uma imagem.\n",
    "* **Embeddings Multimodais**: Representam diferentes tipos de dados (e.g., texto e imagem) em um mesmo espaço vetorial compartilhado, permitindo a comparação e a busca entre modalidades distintas (e.g., CLIP)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d22cc5",
   "metadata": {},
   "source": [
    "### Embeddings de Texto com Modelos Treinados\n",
    "\n",
    "Modelos de linguagem modernos, especialmente os baseados na arquitetura Transformer, são extremamente eficazes na geração de embeddings de texto de alta qualidade. Esses modelos são pré-treinados em vastos corpora textuais, aprendendo a contextualizar palavras e sentenças. A plataforma Hugging Face disponibiliza acesso a milhares desses modelos pré-treinados.\n",
    "\n",
    "Para nossos experimentos, utilizaremos o `BERTimbau`, uma versão do modelo BERT (Bidirectional Encoder Representations from Transformers) pré-treinada especificamente para o português do Brasil. Para simplificar a obtenção dos embeddings de sentenças inteiras, faremos uso da biblioteca `sentence-transformers`, que adapta modelos como o BERT para produzir representações vetoriais de sentenças de forma eficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330d7ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sentence-transformers torch scikit-learn matplotlib seaborn pandas umap-learn -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f00ce9a",
   "metadata": {},
   "source": [
    "### Carregando um Modelo de Embeddings\n",
    "\n",
    "A biblioteca `sentence-transformers` simplifica o processo de carregamento de modelos da Hugging Face e a subsequente geração de embeddings. Ao carregar o modelo `all-MiniLM-L6-v2`, a biblioteca utiliza uma arquitetura pré-configurada que já inclui a camada de *pooling* (geralmente *mean pooling*) no topo das saídas do modelo base. Isso permite agregar os vetores de saída de cada token em um único vetor de dimensão fixa que representa a sentença inteira."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ea275f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "# Verifica se uma GPU está disponível e define o dispositivo\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Carrega o modelo pré-treinado para inglês\n",
    "# 'all-MiniLM-L6-v2' é um modelo popular, rápido e eficaz para similaridade semântica\n",
    "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "model = SentenceTransformer(model_name, device=device)\n",
    "\n",
    "print(f\"Modelo '{model_name}' carregado com sucesso no dispositivo '{device}'.\")\n",
    "print(f\"Dimensão do embedding: {model.get_sentence_embedding_dimension()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4aaa862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gera os embeddings para a frase de exemplo\n",
    "sentence_embeddings = model.encode(\"This is an example sentence for embedding.\")\n",
    "\n",
    "print(f\"Shape da matriz de embeddings: {sentence_embeddings.shape}\")\n",
    "print(f\"Embedding da primeira sentença:\\n{sentence_embeddings[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fda642b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria algumas sentenças de exemplo em inglês\n",
    "sentences = [\n",
    "    \"Natural language processing has advanced greatly.\",\n",
    "    \"Artificial intelligence is transforming the world.\",\n",
    "    \"What is the capital of France?\",\n",
    "    \"Paris is the most populated city in France.\"\n",
    "]\n",
    "\n",
    "sentence_embeddings = model.encode(sentences)\n",
    "\n",
    "print(f\"Shape da matriz de embeddings: {sentence_embeddings.shape}\")\n",
    "print(f\"Embedding da primeira sentença:\\n{sentence_embeddings[0][:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969ac3e1",
   "metadata": {},
   "source": [
    "### Aplicações de Embeddings\n",
    "\n",
    "Com as sentenças representadas como vetores, podemos realizar operações matemáticas para quantificar suas relações semânticas.\n",
    "\n",
    "#### Cálculo de Similaridade Semântica\n",
    "\n",
    "A similaridade de cosseno é uma métrica frequentemente utilizada para medir a similaridade entre dois vetores em um espaço de alta dimensão. Ela calcula o cosseno do ângulo entre os vetores, resultando em um valor que varia de -1 (exatamente opostos) a 1 (exatamente iguais), onde 0 indica ortogonalidade.\n",
    "\n",
    "A fórmula da similaridade de cosseno entre dois vetores $A$ e $B$ é:\n",
    "\n",
    "$$\\text{similarity}(A, B) = \\cos(\\theta) = \\frac{A \\cdot B}{\\|A\\| \\|B\\|} = \\frac{\\sum_{i=1}^{n} A_i B_i}{\\sqrt{\\sum_{i=1}^{n} A_i^2} \\sqrt{\\sum_{i=1}^{n} B_i^2}}$$\n",
    "\n",
    "Valores próximos de 1 indicam alta similaridade semântica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08a52d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575efce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo 1: Sentenças semanticamente similares\n",
    "embedding1 = model.encode([\"What is the capital of the United Kingdom?\"])\n",
    "embedding2 = model.encode([\"London is the main city in the UK.\"])\n",
    "similarity_score = cosine_similarity(embedding1, embedding2)[0][0]\n",
    "print(f\"Similaridade (Capital of UK vs London): {similarity_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5958af21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo 2: Sentenças semanticamente diferentes\n",
    "embedding3 = model.encode([\"Cats are popular pets.\"])\n",
    "embedding4 = model.encode([\"Quantum physics is a complex field.\"])\n",
    "similarity_score_2 = cosine_similarity(embedding3, embedding4)[0][0]\n",
    "print(f\"Similaridade (Cats vs Quantum Physics): {similarity_score_2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f6538a",
   "metadata": {},
   "source": [
    "#### Busca Semântica\n",
    "\n",
    "A busca semântica supera as limitações da busca por palavras-chave. Em vez de buscar correspondências exatas de termos, um sistema de busca semântica encontra documentos que são semanticamente relevantes para uma consulta, mesmo que não compartilhem as mesmas palavras.\n",
    "\n",
    "O processo é direto:\n",
    "1.  **Indexação**: Todo o corpus de documentos é pré-processado e convertido em vetores de embedding, que são armazenados em um banco de dados vetorial ou em memória.\n",
    "2.  **Consulta**: A consulta do usuário é convertida no mesmo formato de embedding.\n",
    "3.  **Recuperação**: O vetor da consulta é comparado com todos os vetores do corpus (usando a similaridade de cosseno), e os documentos mais similares são retornados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31bc53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"Natural language processing has advanced greatly.\",\n",
    "    \"Artificial intelligence is transforming the world.\",\n",
    "    \"What is the capital of France?\",\n",
    "    \"Paris is the most populated city in France.\",\n",
    "    \"I like to cook Italian dishes on Sundays.\",\n",
    "    \"The recipe for pasta carbonara is simple.\",\n",
    "    \"The stock market experienced a drop today.\",\n",
    "    \"Investing in technology can be risky.\"\n",
    "]\n",
    "\n",
    "sentence_embeddings = model.encode(sentences)\n",
    "\n",
    "# Consulta\n",
    "query = \"What to do in Paris?\"\n",
    "query_embedding = model.encode([query])\n",
    "\n",
    "# Recuperação: Calcula a similaridade de cosseno entre a consulta e todas as sentenças do corpus\n",
    "similarities = cosine_similarity(query_embedding, sentence_embeddings)[0]\n",
    "\n",
    "# Encontra as k sentenças mais similares\n",
    "k = 2\n",
    "top_k_indices = np.argsort(similarities)[::-1][:k]\n",
    "\n",
    "print(f\"Consulta: '{query}'\\n\")\n",
    "print(f\"Top {k} sentenças mais similares no corpus:\")\n",
    "for index in top_k_indices:\n",
    "    print(f\"  - {sentences[index]} (Score: {similarities[index]:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57e357c",
   "metadata": {},
   "source": [
    "### Visualização de Embeddings com Redução de Dimensionalidade\n",
    "\n",
    "Os embeddings gerados pelo modelo possuem 384 dimensões, tornando sua visualização direta impossível. Para inspecionar a estrutura do espaço de embeddings, utilizamos algoritmos de redução de dimensionalidade não-linear, como t-SNE e UMAP. O objetivo é projetar os dados em um espaço 2D ou 3D, preservando ao máximo as relações de vizinhança do espaço original.\n",
    "\n",
    "#### t-SNE (t-Distributed Stochastic Neighbor Embedding)\n",
    "\n",
    "O t-SNE é um algoritmo que modela as distâncias entre pontos de dados como probabilidades. No espaço de alta dimensão, ele constrói uma distribuição de probabilidade sobre pares de pontos, de forma que pontos similares tenham uma alta probabilidade de serem escolhidos. Em seguida, ele tenta otimizar um mapeamento para o espaço de baixa dimensão que minimize a divergência de Kullback-Leibler entre as distribuições de probabilidade dos dois espaços. O t-SNE é particularmente eficaz em revelar a estrutura de clusters locais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195c8028",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Aplica o t-SNE para reduzir a dimensionalidade para 2 componentes\n",
    "# A perplexidade pode ser ajustada; um valor entre 5 e 50 é comum.\n",
    "# Como temos poucas amostras, um valor baixo é mais apropriado.\n",
    "tsne = TSNE(n_components=2, perplexity=5, random_state=42, init='random', learning_rate=200)\n",
    "embeddings_2d_tsne = tsne.fit_transform(sentence_embeddings)\n",
    "\n",
    "# Cria um DataFrame para a plotagem\n",
    "df_tsne = pd.DataFrame({\n",
    "    'x': embeddings_2d_tsne[:, 0],\n",
    "    'y': embeddings_2d_tsne[:, 1],\n",
    "    'sentence': sentences\n",
    "})\n",
    "\n",
    "# Plota os resultados\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(data=df_tsne, x='x', y='y', s=150)\n",
    "\n",
    "# Adiciona anotações aos pontos\n",
    "for i in range(df_tsne.shape[0]):\n",
    "    plt.text(df_tsne.x[i]+0.02, df_tsne.y[i], df_tsne.sentence[i],\n",
    "             horizontalalignment='left', size='medium', color='black', weight='semibold')\n",
    "\n",
    "plt.title('Visualização de Embeddings com t-SNE')\n",
    "plt.xlabel('Componente t-SNE 1')\n",
    "plt.ylabel('Componente t-SNE 2')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
