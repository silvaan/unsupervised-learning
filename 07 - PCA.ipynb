{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdef641d",
   "metadata": {},
   "source": [
    "# Análise de Componentes Principais (PCA)\n",
    "\n",
    "Este notebook introduz a Análise de Componentes Principais (PCA), uma técnica fundamental em aprendizado de máquina e estatística para redução de dimensionalidade. O objetivo do PCA é transformar um conjunto de dados com variáveis possivelmente correlacionadas em um novo conjunto de variáveis linearmente não correlacionadas, chamadas de componentes principais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff83928",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2002a17",
   "metadata": {},
   "source": [
    "## Dataset Sintético\n",
    "\n",
    "Para ilustrar os conceitos, geraremos um conjunto de dados bidimensional sintético com uma correlação linear clara entre as duas variáveis. Isso nos permitirá visualizar as métricas e os vetores calculados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865d7d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Gerar dados em torno de uma linha\n",
    "X1 = np.random.rand(100) * 10\n",
    "X2 = 2 * X1 + 3 + np.random.randn(100) * 2\n",
    "\n",
    "# Combinar em uma única matriz de dados\n",
    "X = np.vstack((X1, X2)).T\n",
    "\n",
    "print(\"Dimensões do dataset sintético:\", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae186ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.7)\n",
    "plt.title('Dataset Sintético Original')\n",
    "plt.xlabel('Característica 1')\n",
    "plt.ylabel('Característica 2')\n",
    "plt.axis('equal')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5de0f3b",
   "metadata": {},
   "source": [
    "## Fundamentos\n",
    "\n",
    "A PCA é fundamentada em conceitos da álgebra linear. A técnica busca encontrar um novo sistema de coordenadas, onde os eixos (os componentes principais) são as direções de maior variância nos dados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdce3875",
   "metadata": {},
   "source": [
    "### Covariância\n",
    "\n",
    "A covariância é uma medida da relação linear entre duas variáveis aleatórias. Se as duas variáveis tendem a aumentar ou diminuir juntas, a covariância é positiva. Se uma variável tende a aumentar enquanto a outra diminui, a covariância é negativa. Se não há uma tendência linear, a covariância é próxima de zero.\n",
    "\n",
    "Para duas variáveis $X$ e $Y$ com $m$ amostras, a covariância é calculada como:\n",
    "\n",
    "$$\n",
    "\\text{cov}(X, Y) = \\frac{\\sum_{i=1}^{m} (x_i - \\bar{x})(y_i - \\bar{y})}{m-1}\n",
    "$$\n",
    "\n",
    "Onde $\\bar{x}$ e $\\bar{y}$ são as médias de $X$ e $Y$, respectivamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f79ca04",
   "metadata": {},
   "source": [
    "### Matriz de Covariância\n",
    "\n",
    "Para um conjunto de dados com $n$ características, a matriz de covariância, $\\Sigma$, generaliza a covariância para múltiplas dimensões. É uma matriz quadrada $n \\times n$ que contém as covariâncias entre cada par de características.\n",
    "\n",
    "$$\n",
    "\\Sigma = \\begin{pmatrix}\n",
    "\\text{cov}(X_1, X_1) & \\text{cov}(X_1, X_2) & \\cdots & \\text{cov}(X_1, X_n) \\\\\n",
    "\\text{cov}(X_2, X_1) & \\text{cov}(X_2, X_2) & \\cdots & \\text{cov}(X_2, X_n) \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\text{cov}(X_n, X_1) & \\text{cov}(X_n, X_2) & \\cdots & \\text{cov}(X_n, X_n)\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "A diagonal desta matriz contém as variâncias de cada característica ($\\text{cov}(X_j, X_j) = \\text{var}(X_j)$), e os elementos fora da diagonal representam as covariâncias entre os pares de características. A matriz de covariância é simétrica, pois $\\text{cov}(X_i, X_j) = \\text{cov}(X_j, X_i)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ac568f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Centralizar os dados (subtrair a média)\n",
    "mean = np.mean(X, axis=0)\n",
    "X_centered = X - mean\n",
    "\n",
    "# 2. Calcular a matriz de covariância\n",
    "cov_matrix = np.cov(X_centered, rowvar=False)\n",
    "\n",
    "print(\"Média das características:\", mean)\n",
    "print(\"\\nMatriz de Covariância:\")\n",
    "print(cov_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfdbd5c",
   "metadata": {},
   "source": [
    "### Autovetores e Autovalores\n",
    "\n",
    "Um passo importante do PCA é a decomposição da matriz de covariância em seus autovetores e autovalores. Esta decomposição nos revela as direções principais de variância nos dados.\n",
    "\n",
    "Um autovetor de uma matriz é um vetor não nulo que, quando multiplicado pela matriz, resulta no mesmo vetor original, apenas escalonado por um fator $\\lambda$, que é o autovalor correspondente. A relação é definida por:\n",
    "\n",
    "$$\n",
    "\\Sigma v = \\lambda v\n",
    "$$\n",
    "\n",
    "Onde $\\Sigma$ é a matriz de covariância, $v$ é um autovetor e $\\lambda$ é seu autovalor.\n",
    "\n",
    "- **Autovetores**: Representam as direções dos eixos no novo espaço de características. No contexto do PCA, eles são os **componentes principais**. Eles são ortogonais entre si.\n",
    "- **Autovalores**: Indicam a magnitude da variância dos dados ao longo das direções dos autovetores.\n",
    "\n",
    "O primeiro componente principal é o autovetor associado ao maior autovalor, representando a direção de maior variância nos dados. O segundo componente principal é o autovetor com o segundo maior autovalor, e assim por diante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b623586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular autovetores e autovalores da matriz de covariância\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "\n",
    "print(\"Autovalores:\")\n",
    "print(eigenvalues)\n",
    "print(\"\\nAutovetores (cada coluna é um autovetor):\")\n",
    "print(eigenvectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6140655e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordenar autovetores pelos autovalores\n",
    "sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "eigenvalues_sorted = eigenvalues[sorted_indices]\n",
    "eigenvectors_sorted = eigenvectors[:, sorted_indices]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.5)\n",
    "\n",
    "# Plotar os autovetores com quiver\n",
    "for i in range(len(eigenvalues_sorted)):\n",
    "    eigenvector = eigenvectors_sorted[:, i]\n",
    "    eigenvalue = eigenvalues_sorted[i]\n",
    "    vector_scaled = -eigenvector * np.sqrt(eigenvalue) * 2  # escala proporcional à variância\n",
    "    plt.quiver(\n",
    "        mean[0], mean[1],                # origem\n",
    "        vector_scaled[0], vector_scaled[1],  # direção\n",
    "        angles='xy', scale_units='xy', scale=1,\n",
    "        color='red', width=0.005\n",
    "    )\n",
    "    print(f'Autovetor {i+1}: {eigenvector}, Autovalor: {eigenvalue}')\n",
    "\n",
    "plt.xlim(X[:, 0].min() - 5, X[:, 0].max() + 5)\n",
    "plt.ylim(X[:, 1].min() - 5, X[:, 1].max() + 5)\n",
    "plt.title('Autovetores da Matriz de Covariância (quiver)')\n",
    "plt.xlabel('Característica 1')\n",
    "plt.ylabel('Característica 2')\n",
    "plt.axis('equal')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0129bed8",
   "metadata": {},
   "source": [
    "## Implementação da Classe PCA\n",
    "\n",
    "Com a compreensão dos conceitos fundamentais, agora podemos implementar o PCA utilizando NumPy. Nossa classe `PCA` seguirá uma estrutura similar à encontrada em bibliotecas como o Scikit-learn, contendo os métodos `fit`, `transform` e `fit_transform`. O método `fit` irá encapsular os passos que acabamos de realizar manualmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540851c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCA:\n",
    "    def __init__(self, n_components):\n",
    "        self.n_components = n_components\n",
    "        self.components = None\n",
    "        self.mean = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        # 1. Centralizar os dados (subtrair a média)\n",
    "        self.mean = np.mean(X, axis=0)\n",
    "        X_centered = X - self.mean\n",
    "        \n",
    "        # 2. Calcular a matriz de covariância\n",
    "        # rowvar=False indica que as colunas são as variáveis\n",
    "        cov_matrix = np.cov(X_centered, rowvar=False)\n",
    "        \n",
    "        # 3. Calcular autovetores e autovalores\n",
    "        eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "        \n",
    "        # 4. Ordenar autovetores pelos autovalores em ordem decrescente\n",
    "        # Os autovetores são as colunas da matriz `eigenvectors`\n",
    "        eigenvectors = eigenvectors.T\n",
    "        idxs = np.argsort(eigenvalues)[::-1]\n",
    "        eigenvalues = eigenvalues[idxs]\n",
    "        eigenvectors = eigenvectors[idxs]\n",
    "        \n",
    "        # 5. Armazenar os `n_components` primeiros autovetores\n",
    "        self.components = eigenvectors[0:self.n_components]\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Centralizar os dados usando a média do treino\n",
    "        X_centered = X - self.mean\n",
    "        \n",
    "        # Projetar os dados nos componentes\n",
    "        # (n_samples, n_features) @ (n_features, n_components) -> (n_samples, n_components)\n",
    "        X_projected = np.dot(X_centered, self.components.T)\n",
    "        \n",
    "        return X_projected\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15534f2",
   "metadata": {},
   "source": [
    "### Aplicando o PCA aos Dados\n",
    "\n",
    "Agora, vamos instanciar nossa classe `PCA` e aplicá-la ao conjunto de dados sintético. Reduziremos a dimensionalidade de 2 para 1, retendo apenas o primeiro componente principal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2c98ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciar e aplicar o PCA\n",
    "pca = PCA(n_components=1)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "print(\"Dimensão dos dados originais:\", X.shape)\n",
    "print(\"Dimensão dos dados transformados:\", X_pca.shape)\n",
    "\n",
    "# Para reconstruir os dados (aproximadamente) a partir do espaço reduzido:\n",
    "# X_reconstruido = np.dot(X_pca, pca.components) + pca.mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1461936c",
   "metadata": {},
   "source": [
    "## Aplicação: Iris Dataset\n",
    "\n",
    "Vamos agora aplicar nossa implementação de PCA em um cenário prático. O Iris dataset é um conjunto de dados clássico em machine learning, contendo 150 amostras de flores de íris, cada uma com 4 características (comprimento e largura da sépala e da pétala). O objetivo é classificar cada flor em uma das três espécies possíveis: Setosa, Versicolor e Virginica. Como os dados possuem 4 dimensões, não podemos visualizá-los diretamente. Usaremos o PCA para reduzir a dimensionalidade para 2, permitindo-nos plotar e inspecionar a separação das classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00bd24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Carregando o dataset\n",
    "iris = load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target\n",
    "\n",
    "print(\"Dimensões originais do Iris dataset:\", X_iris.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0720779b",
   "metadata": {},
   "source": [
    "### Padronização dos Dados\n",
    "\n",
    "O PCA é sensível à escala das variáveis. Características com variâncias muito maiores que outras podem dominar os componentes principais, mesmo que não sejam necessariamente as mais informativas para a estrutura dos dados. Para mitigar esse efeito, é uma prática padrão e altamente recomendada padronizar os dados antes de aplicar o PCA. A padronização transforma os dados para que cada característica tenha média zero e desvio padrão unitário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73942e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padronizando as características\n",
    "scaler = StandardScaler()\n",
    "X_iris_scaled = scaler.fit_transform(X_iris)\n",
    "\n",
    "# A média de cada coluna agora é próxima de 0\n",
    "print(\"Média após padronização:\", np.mean(X_iris_scaled, axis=0))\n",
    "# O desvio padrão de cada coluna agora é próximo de 1\n",
    "print(\"Desvio padrão após padronização:\", np.std(X_iris_scaled, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538e3f9d",
   "metadata": {},
   "source": [
    "### Aplicando PCA para Redução de Dimensionalidade\n",
    "\n",
    "Com os dados padronizados, podemos agora aplicar nossa classe `PCA`. Vamos reduzir as 4 dimensões originais para 2, que corresponderão aos dois primeiros componentes principais (aqueles com a maior variância)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4dd40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciar e aplicar o PCA para reduzir de 4 para 2 dimensões\n",
    "pca_iris = PCA(n_components=2)\n",
    "X_iris_pca = pca_iris.fit_transform(X_iris_scaled)\n",
    "\n",
    "print(\"Dimensão dos dados após o PCA:\", X_iris_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc84296",
   "metadata": {},
   "source": [
    "### Visualização dos Dados no Espaço dos Componentes Principais\n",
    "\n",
    "Agora que os dados foram projetados em um espaço bidimensional, podemos criar um gráfico de dispersão. Cada ponto no gráfico representará uma flor, e sua posição será determinada pelos valores do primeiro e do segundo componente principal. Vamos colorir cada ponto de acordo com sua espécie original para verificar se os componentes principais conseguiram capturar uma estrutura que separa as classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61348756",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "target_names = iris.target_names\n",
    "colors = ['navy', 'turquoise', 'darkorange']\n",
    "\n",
    "# Plotar cada classe separadamente para criar a legenda\n",
    "for color, i, target_name in zip(colors, [0, 1, 2], target_names):\n",
    "    plt.scatter(X_iris_pca[y_iris == i, 0], X_iris_pca[y_iris == i, 1],\n",
    "                color=color, alpha=.8, lw=2,\n",
    "                label=target_name)\n",
    "\n",
    "plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "plt.title('PCA do Iris Dataset')\n",
    "plt.xlabel('Primeiro Componente Principal (PC1)')\n",
    "plt.ylabel('Segundo Componente Principal (PC2)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d61b71",
   "metadata": {},
   "source": [
    "## Aplicação: Eigenfaces com o LFW Dataset\n",
    "\n",
    "Uma das aplicações mais famosas do PCA é no campo da visão computacional, especificamente para o reconhecimento facial. A técnica, conhecida como Eigenfaces, trata cada imagem de um rosto como um vetor em um espaço de alta dimensionalidade (onde cada pixel é uma dimensão). O PCA é então utilizado para encontrar um subespaço de dimensionalidade muito menor que captura as variações mais significativas entre os rostos no conjunto de dados. Os componentes principais deste subespaço, quando visualizados como imagens, são chamados de \"Eigenfaces\". Eles representam os \"ingredientes\" fundamentais a partir dos quais todos os rostos do dataset podem ser construídos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f75320",
   "metadata": {},
   "source": [
    "### Carregando o Dataset de Rostos\n",
    "\n",
    "Usaremos o dataset \"Labeled Faces in the Wild\" (LFW), que está convenientemente disponível na biblioteca Scikit-learn. Para tornar o processamento computacionalmente viável para este notebook, vamos carregar um subconjunto pré-processado do dataset, selecionando apenas indivíduos para os quais existem pelo menos 70 fotos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54ae132",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_lfw_people\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Carregar o dataset\n",
    "# min_faces_per_person: Filtra para pessoas com um número mínimo de fotos\n",
    "# resize: Garante que todas as imagens tenham o mesmo tamanho\n",
    "lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n",
    "\n",
    "# Inspecionar as dimensões dos dados\n",
    "n_samples, h, w = lfw_people.images.shape\n",
    "X_faces = lfw_people.data\n",
    "n_features = X_faces.shape[1]\n",
    "y_faces = lfw_people.target\n",
    "target_names = lfw_people.target_names\n",
    "n_classes = target_names.shape[0]\n",
    "\n",
    "print(\"Número total de amostras:\", n_samples)\n",
    "print(\"Número de características (pixels):\", n_features)\n",
    "print(\"Número de classes (pessoas):\", n_classes)\n",
    "print(\"Dimensões de cada imagem: {} x {}\".format(h, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfad8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gallery(images, titles, h, w, n_row=3, n_col=4):\n",
    "    plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))\n",
    "    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n",
    "    for i in range(n_row * n_col):\n",
    "        plt.subplot(n_row, n_col, i + 1)\n",
    "        plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)\n",
    "        plt.title(titles[i], size=12)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "\n",
    "# Plotar a primeira galeria\n",
    "sample_titles = [target_names[y_faces[i]] for i in range(12)]\n",
    "plot_gallery(X_faces, sample_titles, h, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfb7f46",
   "metadata": {},
   "source": [
    "### Dividindo os Dados e Aplicando o PCA\n",
    "\n",
    "Vamos dividir os dados em conjuntos de treino e teste. O PCA será \"treinado\" (ajustado) apenas no conjunto de treino para encontrar os componentes principais (as eigenfaces). O conjunto de teste será usado para avaliar a capacidade do modelo de representar rostos que não foram vistos durante o treinamento.\n",
    "\n",
    "Vamos escolher reter 150 componentes principais, o que representa uma redução de dimensionalidade significativa em relação aos 1850 pixels originais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56629cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir em conjunto de treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_faces, y_faces, test_size=0.25, random_state=42)\n",
    "\n",
    "# Instanciar e aplicar o PCA\n",
    "n_components = 150\n",
    "pca_faces = PCA(n_components=n_components)\n",
    "pca_faces.fit(X_train)\n",
    "\n",
    "print(f\"PCA ajustado. {n_components} componentes principais foram extraídos de {n_features} características.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64549c6b",
   "metadata": {},
   "source": [
    "### Visualizando as Eigenfaces\n",
    "\n",
    "As eigenfaces são simplesmente os componentes principais remodelados para as dimensões originais da imagem. Elas parecem rostos fantasmagóricos e representam as direções de maior variância nos dados. A primeira eigenface captura a variação mais proeminente (geralmente relacionada à iluminação), a segunda captura a próxima maior variação, e assim por diante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae1f987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As eigenfaces são os componentes armazenados no objeto PCA\n",
    "eigenfaces = pca_faces.components.real.reshape((n_components, h, w))\n",
    "\n",
    "eigenface_titles = [\"eigenface %d\" % i for i in range(eigenfaces.shape[0])]\n",
    "plot_gallery(eigenfaces, eigenface_titles, h, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d8384d",
   "metadata": {},
   "source": [
    "### Reconstrução de Rostos a Partir das Eigenfaces\n",
    "\n",
    "A beleza do PCA é que podemos usar os componentes para reconstruir uma aproximação do rosto original. O processo envolve duas etapas:\n",
    "\n",
    "1.  **Projeção (`transform`)**: O rosto original é projetado no subespaço de baixa dimensão, resultando em um vetor de `n_components` pesos. Cada peso indica a \"importância\" de cada eigenface na composição daquele rosto específico.\n",
    "2.  **Reconstrução (Transformação Inversa)**: O rosto aproximado é reconstruído realizando uma combinação linear das eigenfaces, ponderada pelos pesos obtidos na etapa anterior, e adicionando de volta o rosto médio que foi subtraído no início do processo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889825a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Projetar os dados de teste no subespaço das eigenfaces\n",
    "X_test_pca = pca_faces.transform(X_test)\n",
    "\n",
    "# Reconstruir as imagens a partir da representação de baixa dimensão\n",
    "# Reconstrução = (Projeção @ Componentes) + Média\n",
    "X_test_reconstructed = np.dot(X_test_pca, pca_faces.components) + pca_faces.mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029512dd",
   "metadata": {},
   "source": [
    "### Comparando Rostos Originais e Reconstruídos\n",
    "\n",
    "Agora, vamos visualizar alguns rostos do conjunto de teste, lado a lado com suas reconstruções. Isso nos permite avaliar qualitativamente quanta informação foi preservada após a compressão para 150 dimensões."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7486f53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reconstructions(original, reconstructed, titles, h, w):\n",
    "    n_images = len(original)\n",
    "    plt.figure(figsize=(1.8 * 4, 2.4 * (n_images // 2)))\n",
    "    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n",
    "    \n",
    "    for i in range(n_images):\n",
    "        # Plot Original\n",
    "        plt.subplot(n_images, 2, 2*i + 1)\n",
    "        plt.imshow(original[i].real.reshape((h, w)), cmap=plt.cm.gray)\n",
    "        plt.title(\"Original: \" + titles[i], size=12)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "        \n",
    "        # Plot Reconstruído\n",
    "        plt.subplot(n_images, 2, 2*i + 2)\n",
    "        plt.imshow(reconstructed[i].real.reshape((h, w)), cmap=plt.cm.gray)\n",
    "        plt.title(\"Reconstruído\", size=12)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "\n",
    "# Selecionar 5 imagens de teste para visualização\n",
    "n_comparisons = 5\n",
    "test_titles = [target_names[y_test[i]] for i in range(n_comparisons)]\n",
    "plot_reconstructions(X_test[:n_comparisons], X_test_reconstructed[:n_comparisons], test_titles, h, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf2acef",
   "metadata": {},
   "source": [
    "## Exercícios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5c26d2",
   "metadata": {},
   "source": [
    "### Exercício 1: Classificação de Vinhos com PCA\n",
    "\n",
    "O dataset \"Wine\", disponível em `sklearn.datasets`, contém os resultados de uma análise química de vinhos cultivados na mesma região na Itália, mas derivados de três cultivares (classes) diferentes. A análise determinou as quantidades de 13 constituintes encontrados em cada um dos três tipos de vinhos. Sua tarefa é usar o PCA para visualização. Carregue o dataset, padronize as 13 características e, em seguida, aplique o PCA para reduzir a dimensionalidade para 2 componentes. Por fim, crie um gráfico de dispersão dos dados projetados, colorindo cada ponto de acordo com sua classe de vinho original, e observe a eficácia do PCA em separar os diferentes cultivares no espaço bidimensional."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aab1dcc",
   "metadata": {},
   "source": [
    "### Exercício 2: Compressão e Reconstrução com o MNIST\n",
    "\n",
    "O dataset MNIST é um dos mais famosos em machine learning e consiste em 70.000 imagens em escala de cinza de 28x28 pixels de dígitos manuscritos (0-9). Devido à sua maior dimensão (784 features) e tamanho, ele representa um desafio mais realista para a redução de dimensionalidade. Sua tarefa é investigar a capacidade do PCA de comprimir e reconstruir essas imagens. Carregue o dataset (disponível em várias bibliotecas como `sklearn.datasets.fetch_openml`, `tensorflow.keras.datasets` ou `torchvision.datasets`), aplique o PCA no conjunto de treinamento e, em seguida, use os componentes aprendidos para transformar e reconstruir algumas imagens do conjunto de teste. Finalmente, crie uma visualização comparando as imagens originais com suas versões reconstruídas para avaliar qualitativamente a perda de informação."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
