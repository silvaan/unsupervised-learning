{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b371c1d6",
   "metadata": {},
   "source": [
    "# t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "\n",
    "Este notebook explora o t-SNE, uma técnica de redução de dimensionalidade não linear particularmente eficaz para a visualização de datasets de alta dimensão. Diferentemente de métodos como o PCA, que se concentram em preservar a variância global, o t-SNE foca em preservar a estrutura local, ou seja, as similaridades entre pontos vizinhos. Ele modela as similaridades entre pontos de dados tanto no espaço de alta dimensão quanto no de baixa dimensão e busca minimizar a divergência entre essas duas distribuições de similaridade."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157ddb50",
   "metadata": {},
   "source": [
    "## Fundamentação\n",
    "\n",
    "O t-SNE opera em duas etapas principais: primeiro, ele constrói uma distribuição de probabilidade sobre pares de pontos de dados de alta dimensão, de modo que pontos similares tenham uma alta probabilidade de serem escolhidos, enquanto pontos dissimilares tenham uma probabilidade extremamente baixa. Segundo, ele define uma distribuição de probabilidade similar sobre os pontos no mapa de baixa dimensão e minimiza a divergência de Kullback-Leibler (KL) entre as duas distribuições em relação às localizações dos pontos no mapa.\n",
    "\n",
    "### Similaridades no Espaço de Alta Dimensão\n",
    "\n",
    "A similaridade entre os pontos de dados $x_i$ e $x_j$ é a probabilidade condicional, $p_{j|i}$, de que $x_i$ escolheria $x_j$ como seu vizinho se os vizinhos fossem escolhidos em proporção à sua densidade de probabilidade sob uma Gaussiana centrada em $x_i$. A probabilidade é dada por:\n",
    "\n",
    "$$\n",
    "p_{j|i} = \\frac{\\exp(-\\|x_i - x_j\\|^2 / 2\\sigma_i^2)}{\\sum_{k \\neq i} \\exp(-\\|x_i - x_k\\|^2 / 2\\sigma_i^2)}\n",
    "$$\n",
    "\n",
    "A variância da Gaussiana, $\\sigma_i^2$, é determinada para cada ponto de dados individualmente através de uma busca binária. Essa busca é realizada para encontrar um $\\sigma_i$ que produza uma distribuição de probabilidade $P_i$ com uma perplexidade específica, um hiperparâmetro definido pelo usuário. A perplexidade pode ser interpretada como uma medida suave do número efetivo de vizinhos para cada ponto.\n",
    "\n",
    "Para obter uma distribuição de probabilidade conjunta, o t-SNE simetriza essas probabilidades condicionais:\n",
    "\n",
    "$$\n",
    "p_{ij} = \\frac{p_{j|i} + p_{i|j}}{2n}\n",
    "$$\n",
    "\n",
    "onde $n$ é o número de pontos de dados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c838ac3d",
   "metadata": {},
   "source": [
    "### Similaridades no Espaço de Baixa Dimensão\n",
    "\n",
    "No espaço de baixa dimensão (geralmente 2D ou 3D), o t-SNE utiliza uma distribuição t de Student com um grau de liberdade (que é equivalente a uma distribuição de Cauchy) para medir as similaridades entre os pontos de baixa dimensão $y_i$ e $y_j$. O uso de uma distribuição de cauda longa como a t de Student permite que pontos moderadamente distantes no espaço de alta dimensão sejam mapeados para distâncias maiores no espaço de baixa dimensão, ajudando a aliviar o problema de \"crowding\" (aglomeração). A probabilidade conjunta $q_{ij}$ é definida como:\n",
    "\n",
    "$$\n",
    "q_{ij} = \\frac{(1 + \\|y_i - y_j\\|^2)^{-1}}{\\sum_{k \\neq l} (1 + \\|y_k - y_l\\|^2)^{-1}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6fbc60",
   "metadata": {},
   "source": [
    "### Otimização via Minimização da Divergência KL\n",
    "\n",
    "O objetivo do t-SNE é fazer com que a distribuição $Q$ (no espaço de baixa dimensão) reflita a distribuição $P$ (no espaço de alta dimensão) da melhor forma possível. Isso é alcançado minimizando a divergência de Kullback-Leibler entre as duas distribuições:\n",
    "\n",
    "$$\n",
    "C = \\text{KL}(P||Q) = \\sum_{i} \\sum_{j} p_{ij} \\log \\frac{p_{ij}}{q_{ij}}\n",
    "$$\n",
    "\n",
    "A minimização desta função de custo é realizada usando o método do gradiente descendente. O gradiente da divergência KL em relação aos pontos no embedding $y_i$ possui uma forma surpreendentemente simples, que é a diferença entre as forças de atração (puxando pontos similares para perto) e as forças de repulsão (empurrando pontos dissimilares para longe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848138b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0dac79",
   "metadata": {},
   "source": [
    "### Gerando um Dataset Sintético\n",
    "\n",
    "Para demonstrar o t-SNE, vamos primeiro gerar um conjunto de dados sintético com clusters bem definidos. Usaremos a função `make_blobs` da biblioteca `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a850bf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 300\n",
    "n_features = 20\n",
    "n_clusters = 5\n",
    "\n",
    "X, y = make_blobs(\n",
    "    n_samples=n_samples,\n",
    "    n_features=n_features,\n",
    "    centers=n_clusters,\n",
    "    random_state=42,\n",
    "    cluster_std=5.0\n",
    ")\n",
    "\n",
    "print(\"Formato do dataset X:\", X.shape)\n",
    "print(\"Formato do vetor y:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c34e858",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X[:, :8], columns=[f'feature_{i+1}' for i in range(8)])\n",
    "df['cluster'] = y\n",
    "\n",
    "sns.pairplot(df, hue='cluster', palette='hls')\n",
    "plt.suptitle(\"Pairplot das primeiras features (sem t-SNE)\", y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c59caa9",
   "metadata": {},
   "source": [
    "## Implementação da Classe t-SNE\n",
    "\n",
    "Agora, vamos implementar o algoritmo t-SNE, encapsulando a lógica em uma classe Python com uma interface similar à do `scikit-learn`. Esta implementação se concentrará na clareza do algoritmo, em vez de otimizações de performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559156f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSNE:\n",
    "    def __init__(self, n_components=2, perplexity=30.0, lr=200.0, n_iter=500):\n",
    "        # Número de dimensões finais, perplexidade, taxa de aprendizado e iterações\n",
    "        self.n_components = n_components\n",
    "        self.perplexity = perplexity\n",
    "        self.lr = lr\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 1. Calcula matriz de distâncias euclidianas ao quadrado\n",
    "    # -------------------------------------------------------\n",
    "    def _distances(self, X):\n",
    "        sum_X = np.sum(X**2, axis=1)\n",
    "        return np.add(np.add(-2 * X @ X.T, sum_X).T, sum_X)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2. Calcula a matriz P (probabilidades no espaço de alta dimensão)\n",
    "    #    Cada linha P[i] é ajustada para ter uma entropia que corresponda à perplexidade\n",
    "    # -------------------------------------------------------------------------\n",
    "    def _p_matrix(self, D):\n",
    "        n = D.shape[0]\n",
    "        P = np.zeros((n, n))\n",
    "        logU = np.log2(self.perplexity)\n",
    "\n",
    "        for i in range(n):\n",
    "            beta = 1.0  # inverso da variância (1 / (2*sigma²))\n",
    "            Di = np.delete(D[i], i)  # remove a distância com ele mesmo\n",
    "\n",
    "            # Ajuste de beta por busca simples até alcançar a perplexidade desejada\n",
    "            for _ in range(30):\n",
    "                P_i = np.exp(-Di * beta)\n",
    "                P_i /= np.sum(P_i)\n",
    "                H = -np.sum(P_i * np.log2(P_i + 1e-12))  # entropia\n",
    "                if abs(H - logU) < 1e-3:  # se já está próximo, para\n",
    "                    break\n",
    "                beta *= 1.2 if H > logU else 0.8  # ajusta beta\n",
    "\n",
    "            P[i, np.arange(n) != i] = P_i\n",
    "\n",
    "        # Simetriza e normaliza\n",
    "        P = (P + P.T) / (2 * n)\n",
    "        return np.maximum(P, 1e-12)\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 3. Executa o t-SNE e retorna o embedding 2D\n",
    "    # -------------------------------------------------------\n",
    "    def fit_transform(self, X):\n",
    "        n = X.shape[0]\n",
    "\n",
    "        # Calcula afinidades no espaço original (P)\n",
    "        D = self._distances(X)\n",
    "        P = self._p_matrix(D)\n",
    "\n",
    "        # Inicializa o embedding aleatoriamente\n",
    "        Y = np.random.randn(n, self.n_components)\n",
    "\n",
    "        # Loop de otimização\n",
    "        for it in range(self.n_iter):\n",
    "            # Calcula afinidades no espaço de baixa dimensão (Q)\n",
    "            sum_Y = np.sum(Y**2, axis=1)\n",
    "            num = 1 / (1 + np.add(np.add(-2 * Y @ Y.T, sum_Y).T, sum_Y))\n",
    "            np.fill_diagonal(num, 0)\n",
    "            Q = np.maximum(num / np.sum(num), 1e-12)\n",
    "\n",
    "            # Calcula o gradiente\n",
    "            PQ = P - Q\n",
    "            for i in range(n):\n",
    "                # Soma ponderada das diferenças (força de atração/repulsão)\n",
    "                dY_i = np.sum((PQ[:, i] * num[:, i])[:, None] * (Y[i] - Y), axis=0)\n",
    "                Y[i] -= self.lr * dY_i  # Atualiza posição\n",
    "\n",
    "            # Centraliza os pontos\n",
    "            Y -= np.mean(Y, axis=0)\n",
    "\n",
    "            # Mostra custo a cada 100 iterações\n",
    "            if (it + 1) % 100 == 0:\n",
    "                cost = np.sum(P * np.log(P / Q))\n",
    "                print(f\"Iter {it+1:4d}: cost = {cost:.4f}\")\n",
    "\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515f8a7d",
   "metadata": {},
   "source": [
    "### Aplicando\n",
    "\n",
    "Vamos agora instanciar nossa classe `TSNE` e aplicá-la ao mesmo dataset sintético. O processo de otimização será impresso, mostrando o valor da função de custo em intervalos regulares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35965ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_tsne = TSNE(n_components=2, perplexity=30.0, n_iter=1000, lr=200)\n",
    "X_embedded_custom = custom_tsne.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03323223",
   "metadata": {},
   "source": [
    "### Visualizando o Resultado da Implementação\n",
    "\n",
    "Finalmente, plotamos o resultado gerado pela nossa implementação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db8e63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(\n",
    "    x=X_embedded_custom[:, 0],\n",
    "    y=X_embedded_custom[:, 1],\n",
    "    hue=y,\n",
    "    palette=sns.color_palette(\"hls\", n_clusters),\n",
    "    legend=\"full\"\n",
    ")\n",
    "plt.title('Visualização t-SNE com Implementação Customizada')\n",
    "plt.xlabel('Componente t-SNE 1')\n",
    "plt.ylabel('Componente t-SNE 2')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1e7e5c",
   "metadata": {},
   "source": [
    "## Exercícios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40eccf56",
   "metadata": {},
   "source": [
    "### Exercício 1\n",
    "\n",
    "Treine diferentes modelos t-SNE em 2D com diferentes valores de perplexidade para o dataset sintético e plote cada um deles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dba439",
   "metadata": {},
   "source": [
    "### Exercício 2\n",
    "\n",
    "Faça um treinamento de um modelo t-SNE para o Dataset Wine. Em seguida, plote o resultado em duas dimensões."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
